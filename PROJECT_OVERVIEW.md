# SmartStudy MathsMate â€” Project Overview

## Title of the Project
SmartStudy MathsMate: An Interactive, Multiâ€‘Provider AI Learning Assistant for Class 10 Mathematics

## Abstract
SmartStudy MathsMate is a mathâ€‘first learning web application designed to help Class 10 students learn faster with clearer explanations, interactive practice, and stepâ€‘byâ€‘step solutions. The system combines a modern React + TypeScript + Tailwind frontend with an Express backend that can route requests across multiple AI providers (Gemini as primary; OpenAI and Ollama as optional alternatives) and local ruleâ€‘based solvers for certain problems. To ensure reliability in realâ€‘world usage, the backend implements an inâ€‘memory cache, perâ€‘topic cooldown/backoff for 429/503 provider responses, and a structured error shape with retry hints. Students can now also explicitly choose which provider powers an explanation or solution using a simple query parameter, or they can leave it in auto mode for an intelligent fallback cascade. The approach prioritizes accessibility (local fallbacks), resilience (cooldowns, cached explanations), and clarity (Classâ€‘10â€‘friendly style with LaTeX where relevant), delivering a practical, extensible foundation for a math learning companion.

An interactive, mathâ€‘first learning app focused on Class 10 mathematics. It combines a modern React UI with an Express backend and multiple AI providers to deliver simple explanations and stepâ€‘byâ€‘step solutions.

## What this document covers
- Highâ€‘level architecture (frontend + backend)
- Project layout and key files
- AI provider system (auto + manual selection)
- API endpoints and responses
- Environment variables and configuration
- Running locally (frontend + backend)
- Caching, backoff, and error handling
- Current state vs. nearâ€‘term improvements

---

## Overview of the Project
SmartStudy MathsMate offers:
- Topic explanations written simply for Class 10 students
- Stepâ€‘byâ€‘step solutions with formulas and reasoning
- A topic grid and subject navigation optimized for Mathematics
- A robust AI provider system with both auto fallback and explicit provider selection (`auto|gemini|openai|ollama|local`)
- Local ruleâ€‘based solvers for specific tasks (e.g., terminating decimal expansion checks) to provide answers without cloud APIs

Pedagogically, the app aims to reduce cognitive load by providing succinct explanations, examples, and consistent visual language. Technically, it focuses on resilience (cooldowns/backoff) and adaptability (multiple providers, validated Gemini model at startup).

---

## Architecture at a glance
- Frontend: React 18 + TypeScript + Vite + Tailwind CSS
- Backend: Node.js (Express) + `node-fetch`
- AI Providers:
  - Gemini (primary)
  - OpenAI (optional fallback for explanations)
  - Ollama (optional local provider)
  - Local ruleâ€‘based solvers for some math prompts
- Resilience: Inâ€‘memory cache, perâ€‘topic cooldown/backoff, structured 429/503 responses

```
Browser (Vite React)  <â€”â€”>  Express server (Node)
                              â”œâ”€ Gemini (primary)
                              â”œâ”€ OpenAI (optional fallback)
                              â”œâ”€ Ollama (optional local)
                              â””â”€ Local rule-based solvers
```

### Expanded block diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Frontend (Vite React)                   â”‚
â”‚  - Routes: Home, Subject, MathTopic, Practice, Tips, Progress        â”‚
â”‚  - UI: Tailwind components, LaTeX rendering                          â”‚
â”‚  - Calls: /api/explain, /api/solve (+ ?provider=auto|gemini|...)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ HTTP/JSON
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            Backend (Express)                          â”‚
â”‚  - Provider selection: auto or explicit                               â”‚
â”‚  - Caching: per-topic (30 min)                                        â”‚
â”‚  - Backoff: per-topic cooldown on 429/503                             â”‚
â”‚  - validateGeminiModel at startup                                     â”‚
â”‚  - tryLocalSolve for specific prompts                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚               â”‚                       â”‚
    â”‚               â”‚                       â”‚
  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
  â”‚  Gemini   â”‚   â”‚  OpenAI   â”‚           â”‚  Ollama   â”‚
  â”‚ (primary) â”‚   â”‚ (optional)â”‚           â”‚ (optional)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                                   
        â”‚
         Local rule-based solvers (no network)
```

---

## Repository structure (relevant)
```
eslint.config.js
index.html
package.json            # frontend package.json with vite scripts
postcss.config.js
README.md               # quick intro
tailwind.config.js
vite.config.ts

server/
  package.json          # backend package.json (start/dev scripts)
  server.js             # Express server and AI endpoints

src/
  App.tsx               # routes wiring
  index.css, main.tsx
  components/
    Navbar.tsx, Footer.tsx
  pages/
    Home.tsx, Subject.tsx, TipsAndTricks.tsx
    (MathTopic.tsx, Progress.tsx, Practice.tsx are referenced in App routes)
```

---

## Frontend (React + Vite)
- Routes (see `src/App.tsx`):
  - `/` Home
  - `/subject/:subjectName` Subject landing (e.g., maths)
  - `/subject/maths/:topicSlug` Math topic page (detail)
  - `/tips` Tips & Tricks
  - `/progress` Progress tracking (route present)
  - `/practice` Practice (route present)
- UI:
  - `Navbar.tsx` with responsive menu
  - `Footer.tsx` with gradient, subtle animations, and social placeholders
  - `Home.tsx` marketingâ€‘style sections and CTA
  - `Subject.tsx` topic listing and fetch for explanations
- Styling: TailwindCSS via Vite pipeline

Note: Provider selection UI on the frontend is a planned improvement. Backend support already exists; see the Provider Selection section.

---

## Methodology
This project follows a â€œmathâ€‘first, reliabilityâ€‘firstâ€ methodology:

1) Problem framing and UX
- Scope the audience (Class 10) and ensure explanations are short, clear, and exampleâ€‘driven.
- Keep interaction costs low (singleâ€‘click explanations and solver inputs).

2) Multiâ€‘provider strategy
- Primary: Gemini for both explanations and solver.
- Fallbacks: OpenAI (explanations and solver on explicit request) and Ollama (local model) to maintain continuity during provider outages.
- Local ruleâ€‘based solvers answer common checks without network calls.

3) Resilience and consistency
- Perâ€‘topic caching (~30 minutes) prevents repeated calls and stabilizes UI.
- Perâ€‘topic cooldown/backoff avoids rapid retries on 429/503 and communicates `retryAfterSeconds` to the UI.
- Structured response shapes: always include `provider` and sometimes `requestedProvider`.

4) Implementation patterns
- Startup validation: `validateGeminiModel()` lists available models and autoâ€‘resolves a compatible name (e.g., prefer `flash`).
- Provider selection contract: `?provider=auto|gemini|openai|ollama|local` on both endpoints.
- Auto mode cascade: try Gemini â†’ OpenAI (if configured) â†’ Ollama â†’ local.

5) Verification
- Manual tests via curl for each provider option.
- Local solver sanity tests (terminating decimal expansion prompt family).
- Lightweight endâ€‘toâ€‘end checks from the UI pages.

### Minimal providerâ€‘selection flow (pseudo)
```
requested = query.provider || 'auto'
if requested == 'local':
  return localResult()
if requested == 'gemini':
  return callGemini()
if requested == 'openai':
  return callOpenAI()
if requested == 'ollama':
  return callOllama()

# auto
try callGemini()
catch 429/503:
  backoff(topic)
  try openai; else try ollama; else local
catch other:
  try openai; else try ollama; else local
```

---

## Backend (Express)
File: `server/server.js`

### Key helpers
- `callGemini(promptText)` â€” primary provider call
- `callOpenAIChat(system, user)` â€” OpenAI chat fallback (explanations + solver when explicitly requested)
- `callOllama(promptText)` â€” local model call (if configured)
- `validateGeminiModel()` â€” runs at startup, verifies model list and autoâ€‘resolves a working model name when needed
- `parseRetryAfterSeconds(resp, body)` â€” parses Retryâ€‘After from headers/body
- `localExplain(subject, topic)` â€” friendly placeholder/fallback text
- `tryLocalSolve(prompt)` â€” small ruleâ€‘based solvers for some math prompts (e.g., â€œterminating decimal expansionâ€ checks)

### Caching & backoff
- Inâ€‘memory cache: 30â€‘minute TTL per subject/topic
- Perâ€‘topic backoff map: after Gemini returns 429/503, subsequent requests are cooled down with a serverâ€‘side timer; UI can surface `retryAfterSeconds`

---

## Provider selection (auto or explicit)
Backend supports an explicit provider choice via `?provider=` on both endpoints. Allowed values:
- `auto` (default): Gemini â†’ OpenAI (if configured) â†’ Ollama â†’ local
- `gemini`
- `openai`
- `ollama`
- `local`

The JSON response includes two fields:
- `provider`: the provider actually used
- `requestedProvider`: what the client asked for (present when applicable)

You can also force Gemini evaluation via `?force=gemini` on the explanation endpoint to skip cooldown logic.

### Sequence (auto mode)
```
Client â†’ /api/explain?provider=auto
  Server tries Gemini
    â”œâ”€ success â†’ return {provider:'gemini'}
    â””â”€ 429/503 â†’ set backoff â†’ try OpenAI â†’ try Ollama â†’ else local
```

---

## API endpoints

### GET `/api/explain/:subject/:topic`
Returns a concise explanation suitable for Class 10 students.

Query params:
- `provider` = `auto|gemini|openai|ollama|local` (default: `auto`)
- `force=gemini` â€” bypass backoff for this call (not recommended during active cooldowns)

Success (example):
```json
{
  "explanation": "...",
  "cached": true,
  "provider": "gemini",
  "requestedProvider": "auto"
}
```

Rateâ€‘limited/overloaded fallback:
```json
{
  "explanation": "Local or alternate fallback text...",
  "cached": false,
  "provider": "local",
  "requestedProvider": "auto",
  "rateLimited": true,
  "retryAfterSeconds": 45
}
```

No Gemini key and `provider=gemini`:
```json
{
  "explanation": "This is a placeholder explanation... Configure your Gemini API key...",
  "cached": false,
  "provider": "local",
  "requestedProvider": "gemini"
}
```

Example calls:
```bash
# Auto (default): Gemini â†’ OpenAI â†’ Ollama â†’ local
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=auto"

# Explicit providers
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=gemini"
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=openai"
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=ollama"
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=local"
```

### POST `/api/solve`
Body:
```json
{ "prompt": "Check whether 13/3125 has a terminating decimal expansion" }
```

Query params:
- `provider` = `auto|gemini|openai|ollama|local` (default: `auto`)

Success (examples):
```json
{ "solution": "...", "provider": "gemini", "requestedProvider": "auto" }
{ "solution": "...", "provider": "ollama", "requestedProvider": "ollama" }
{ "solution": "...", "provider": "local",  "requestedProvider": "local" }
```

On provider rateâ€‘limit/overload (429/503) in auto mode, server returns structured guidance and may attempt fallbacks before responding with:
```json
{
  "error": "RATE_LIMIT", // or OVERLOADED
  "message": "Model is temporarily unavailable. Please wait and try again.",
  "retryAfterSeconds": 60,
  "detail": "...provider raw message...",
  "provider": "gemini",
  "requestedProvider": "auto"
}
```

Local ruleâ€‘based solver can directly answer some prompts (for faster, keyless responses), for example: terminating decimal checks.

---

## Detailed Results
This section summarizes functional results and how to reproduce them. Since this application is not a classification model, a confusion matrix is not applicable.

### Functional outcomes
- Explanations (auto): When a valid Gemini key is configured, requests return concise, Classâ€‘10â€‘friendly text. If Gemini is unavailable, the system falls back (OpenAI/Ollama/local) and includes `provider`/`requestedProvider` fields.
- Local solver: For prompts like â€œCheck whether 13/3125 has a terminating decimal expansion,â€ the server instantly returns a step list without calling external APIs.
- Provider override: `?provider=gemini|openai|ollama|local` selects an explicit provider. Responses reflect the requested and actual provider.

Example responses (abbreviated):
```json
{"explanation":"...","cached":true,"provider":"gemini","requestedProvider":"auto"}
{"explanation":"...","cached":false,"provider":"local","requestedProvider":"local"}
{"solution":"...","provider":"ollama","requestedProvider":"ollama"}
```

### Suggested graphs (how to generate)
If you wish to produce graphs, consider:
- Response time per provider (ms) for explain/solve
- Cache hit ratio over time
- Rateâ€‘limit events vs. successful calls

You can collect data by wrapping fetch calls with timers in a small Node script or adding simple logging in `server/server.js`, then visualize with your preferred tool (e.g., Chart.js, Python/matplotlib). Avoid fabricating numbers; record them on your machine and state the environment (CPU, RAM, macOS version, Node version).

### Confusion matrix (operational)
While a traditional ML confusion matrix applies to classification tasks, we can present a practical â€œrequested vs. used providerâ€ matrix summarizing how the backend routed recent requests. Counts below reflect observed runs during local testing (both endpoints combined: explain + solve). Use it as an operational reliability view, not a classifier metric.

Requested vs. Used provider (counts)

| Requested â†“ \ Used â†’ | gemini | openai | ollama | local | error |
|----------------------:|:------:|:------:|:------:|:-----:|:-----:|
| auto                  |   1    |   0    |   0    |   0   |   0   |
| gemini                |   0    |   0    |   0    |   0   |   0   |
| openai                |   0    |   0    |   0    |   0   |   0   |
| ollama                |   0    |   0    |   0    |   0   |   0   |
| local                 |   0    |   0    |   0    |   2   |   0   |

Notes
- The two â€œlocal â†’ localâ€ entries came from (1) GET /api/explain with `?provider=local` and (2) POST /api/solve with `?provider=local` for a terminating-decimal check.
- The â€œauto â†’ geminiâ€ entry came from GET /api/explain with `?provider=auto` (Gemini key present), which returned a cached Gemini explanation.
- No explicit `gemini|openai|ollama` runs were recorded during the latest test pass; cells remain 0 until you run them.

How to regenerate locally
```bash
# Auto â†’ expect gemini (if key valid)
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=auto" | jq -r '.provider'

# Explicit providers
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=gemini" | jq -r '.provider'
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=openai" | jq -r '.provider'
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=ollama" | jq -r '.provider'
curl -s "http://localhost:3001/api/explain/maths/Probability?provider=local" | jq -r '.provider'

# Solver examples
curl -s -X POST "http://localhost:3001/api/solve?provider=auto"   -H "Content-Type: application/json" -d '{"prompt":"Check whether 13/3125 has a terminating decimal expansion"}' | jq -r '.provider'
curl -s -X POST "http://localhost:3001/api/solve?provider=local"  -H "Content-Type: application/json" -d '{"prompt":"Check whether 13/3125 has a terminating decimal expansion"}' | jq -r '.provider'
```
Tally each `(requestedProvider, provider)` pair from the JSON and update the table above. If a request fails with a nonâ€‘200 status, increment the `error` column.

### Output screenshots (what to capture)
- Home page hero and feature tiles
- Subject â†’ Mathematics category view with topic cards
- A MathTopic page showing an explanation, andâ€”if addedâ€”the provider selector
- Example solver result (local ruleâ€‘based solution)

You can embed images in this document later by adding them to a `docs/` folder and referencing them:
```
![Maths Topic Grid](docs/maths-topic-grid.png)
![Explanation Panel](docs/explanation-panel.png)
```

---

## Environment configuration (`server/.env`)
Required/optional variables:
```env
# Primary provider (required for real AI explanations/solutions)
GEMINI_API_KEY=your_gemini_key
# Optional: version/model (validated at startup)
GEMINI_API_VERSION=v1beta
GEMINI_MODEL=gemini-1.5-flash

# Optional: explanation/solver alternative providers
OPENAI_API_KEY=your_openai_key
OPENAI_MODEL=gpt-4o-mini

# Optional: local provider (Ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Optional: server port
PORT=3001
```
Startup logs will display the resolved Gemini model:
```
ğŸ¤– Gemini model in use: gemini-1.5-flash (version v1beta)
```
If the configured model is unavailable, the server attempts to autoâ€‘resolve a compatible one via `ListModels`.

---

## Running locally
From repository root:

```bash
# 1) Install
npm install
(cd server && npm install)

# 2) Backend (in a terminal)
cd server
node server.js
# or with nodemon:
npm run dev

# 3) Frontend (in another terminal)
npm run dev
```

- Frontend: http://localhost:5173/
- API:      http://localhost:3001/

Tip: The root `package.json` also includes convenience scripts:
- `npm run server` â†’ `cd server && node server.js`
- `npm run dev:server` â†’ `cd server && nodemon server.js`

---

## Error handling, caching, and cooldowns
- 429 (rate limit) and 503 (overloaded) are detected and mapped to structured JSON with `retryAfterSeconds`.
- In auto mode, the server will attempt provider fallbacks when possible before returning an error payload.
- Explanations are cached in memory for ~30 minutes to reduce provider calls; the cache key is `subject-topic`.
- A perâ€‘topic backoff map prevents hammering a provider during cooldowns.

What youâ€™ll see in the UI when no key is configured (or during cooldown): a clear placeholder explanation like the screenshot you shared, encouraging you to add keys or retry.

---

## Current state vs. next steps
- Implemented:
  - Provider selection via `?provider=` on both endpoints
  - Provider tagging in responses: `provider` and `requestedProvider`
  - Local fallbacks and ruleâ€‘based solvers for certain prompts
  - Gemini model autoâ€‘validation at startup
- Nearâ€‘term improvements:
  - Add UI controls to choose provider on MathTopic and Solver views
  - Display â€œRequested vs. Usedâ€ provider badge when they differ
  - Optional oneâ€‘command dev runner (use `concurrently`) and Vite proxy

---

## Conclusion
SmartStudy MathsMate demonstrates a pragmatic approach to AIâ€‘assisted learning: prioritize clarity for students, combine multiple providers for reliability, and incorporate local fallbacks to remain useful even without cloud access. The provider selection mechanism empowers users to choose the engine behind their explanations or solutions, while auto mode balances convenience and resilience with caching and cooldowns. Future work includes surfacing provider controls in the UI, expanding ruleâ€‘based solvers, adding richer interactive math tools, and building lightweight telemetry to generate performance graphs. Together, these improvements will make the app even more dependable and helpful for students preparing for exams.

---

## Troubleshooting
- "Placeholder explanation" keeps showing:
  - Ensure `server/.env` contains `GEMINI_API_KEY` and the server has been restarted.
- Port already in use:
  - Change the `PORT` or free the port (3001 for API, 5173 for Vite).
- 429/503 repeatedly:
  - Respect cooldowns indicated by `retryAfterSeconds`, or switch provider to `openai`, `ollama`, or `local` temporarily.

---

Made with care to make maths learning simple, visual, and reliable.
